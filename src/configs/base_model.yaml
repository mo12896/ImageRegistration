META:
  EXPERIMENT_NAME: 'baseline'
  USER: 'Moritz'


SETUP:
  DEVICE: 0
  LOGLEVEL: 'INFO'
  MODEL: 'UNet'
  DATASET: 'EMPIRE10'
  TRAIN: True
  TEST: False
  TRAIN_TEST: False
  SPLIT: 0.33
  RANDOM_SEED: 42


HYPERPARAMETERS:
  LEARNING_RATE: 0.01
  PATIENCE: 10
  # note: regularly check wandb logs for GPU Memory Access -> increase/decrease batch_size
  #TODO: find a way in Dataloader, to set bigger batchsizes!
  BATCH_SIZE: 2
  EPOCHS: 200
  EVAL_EVERY: 2
